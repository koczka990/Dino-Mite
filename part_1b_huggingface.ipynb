{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path 2 - HuggingFace\n",
    "HuggingFace (HF) is a free platform where user can upload models (of various kinds, not just LLMs) that can then be used through their `transformers` library. To be able to use the models on HF you don't need to create an account, however, some models are 'gated' and require approval from the creator before being able to use them (it is the case e.g. for LLaMA models). For those models, it's required both authentication and authorization to use the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First simple generation\n",
    "For the means of this lab, we will use the model `Qwen/Qwen2-VL-2B-Instruct`, which is a non-gated fairly small model that, besides text, also support images and videos. For the assignment and the project you can choose the model that you prefer from the [HF catalogue](https://huggingface.co/models).\n",
    "\n",
    "**NOTE:** Until the next release of `transformers`, the use of Qwen VL requires installation from source (see `requirements.txt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, GenerationConfig\n",
    "\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "# We're using the `Qwen2VLForConditionalGeneration` class to enable multimodal generation\n",
    "# Normally, you can use AutoModelForCausalLM\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",  # automatically uses right precision based on model\n",
    "    device_map=\"auto\"  # automatically uses right device e.g. GPU if available\n",
    ")\n",
    "\n",
    "# We're using the `AutoProcessor` class to enable multimodal generation\n",
    "# Normally, you can use AutoTokenizer\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Start with using the model to predict the next part in a conversation. You need to  tokenize the input, generate the response, detokenize it and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",  # optional, could start directly with user\n",
    "        \"content\": \"You are a helpful pirate. Only reply with pirate jargon.\",  # system prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello\",  # user query\n",
    "    },\n",
    "]\n",
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generation parameters\n",
    "When asking the model to generate some text, there are different parameters that you can tune to improve on the final quality of the text. [Here](https://huggingface.co/docs/transformers/generation_strategies) is an overview of the parameters that you can change. Try some of them in different context and understand how they affect the final generated text. Feel also free to explore different decoding strategies.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Play with the output temperature, which controls the randomness of the generated text `temperature=0` means deterministic output, while `temperature=1` means maximum randomness (try some intermediate value too) and keep the `max_output_tokens` to 50 so that the output is not too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Try out different `top_k` values, which controls how many tokens the model considers for output `top_k=1` means the model considers only one token for output (the one with the highest probability) `top_k=50` means the model considers the top 50 tokens for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "The same exercise as before but now with `top_p`, which controls how the model selects tokens for output `top_p=0.1` means the model selects tokens that make up 10% of the cumulative probability mass `top_p=0.9` means the model selects tokens that make up 90% of the cumulative probability mass `top_p` filters tokens *after* applying `top_k`.\n",
    "\n",
    "Can you determine a rule of thumb as to how `top_k` and `top_p` affect the output results? (If you can't try to push the values to extreme values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add images to the prompt\n",
    "This model, beside text also accepts images (and videos).\n",
    "\n",
    "\n",
    "#### Exercise 5\n",
    "Try prompting it with one. Choose an interesting image and prompt the model with a query about it.\n",
    "\n",
    "You can use the model's [README](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct).\n",
    "\n",
    "Use [PIL](https://pillow.readthedocs.io/en/stable/) to load an image. It should already be present in the Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"./data/engineer_fitting_prosthetic_arm.jpg\"\n",
    "\n",
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Document grounding\n",
    "\n",
    "#### Exercise 6 (optional)\n",
    "\n",
    "Depending on the application of the project, you might need to extract text from given documents and include it as additional context. In this case of HuggingFace, you'll need to use external libraries to achieve it more easily. Here are some libraries that you can use: [LangChain](https://python.langchain.com/v0.2/docs/introduction/), [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/), [Haystack](https://docs.haystack.deepset.ai/docs/intro).\n",
    "\n",
    "For the solution of this lab we will use LangChain.\n",
    "\n",
    "**NOTE:** This part is here only to mirror the lab's Path 1 and is *optional*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATH = \"./data/chain_of_thought_prompting.pdf\"\n",
    "\n",
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a user interface\n",
    "\n",
    "#### Exercise 7\n",
    "\n",
    "Since you are trying to build a complete application, you also need a nice user interface that interacts with the model. There are various libraries available for this purpose. Notably: [gradio](https://www.gradio.app/docs/gradio/interface) and [chat UI](https://huggingface.co/docs/chat-ui/index). For the solution of this lab, we will use gradio.\n",
    "\n",
    "Gradio has pre-defined input/output blocks that are automatically inserted in the interface. You only need to provide an appropriate function that takes all the inputs and returns the relevant output. See documentation [here](https://www.gradio.app/docs/gradio/interface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "\n",
    "# This part closes the demo server if it is already running (which\n",
    "# happens easily in notebooks) and prevents you from opening multiple\n",
    "# servers at the same time.\n",
    "if \"demo\" in locals() and demo.is_running:\n",
    "    demo.close()\n",
    "\n",
    "# Edit the parameters below\n",
    "demo = gr.ChatInterface(...)\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
